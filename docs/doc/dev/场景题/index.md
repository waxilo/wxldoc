## 如何设计一个秒杀系统

| 步骤      | 其他                         |
| ------- | -------------------------- |
| Redis预热 | 预先加载商品库存信息                 |
| 库存扣减    | 依赖Redis的库存扣减               |
| MQ异步建单  | Redis扣减后，将购物信息发送到MQ，进行异步建单 |
| DB核对    | 核对库存                       |
|         |                            |
## OOM问题排查

### 确认OOM问题类型

从异常信息中可以看到是什么类型的OOM

| 异常信息                       | 类型     | 排查方向                   |
| -------------------------- | ------ | ---------------------- |
| Java heap space            | 堆空间不足  | 内存泄露、对象优化、缓存设计优化、堆大小调整 |
| Metaspace                  | 元空间溢出  | 动态生成类优化、元空间大小调整        |
| GC overhead limit exceeded | GC效率低下 | 内存泄露、对象创建过速、大对象处理不当    |
### DUMP文件获取

JVM参数设置自动dump文件生成
```
-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=
```

也可以手动触发，但是会触发STW：
```shell
jmap -dump:file=heapdump.hprof <pid> // <pid> 是 Java 进程的进程 ID。
```

当发生OOM，堆、元空间都会生成`.hprof`文件，如果没有，则确认是直接内存OOM

### 分析与定位

拿到 `.hprof` 文件后，使用分析工具（推荐 **MAT** 或 **VisualVM**，Arthas 也是神器）进行分析


## CPU高问题排查

### 1. 定位Java进程

1. **使用 `top` 命令：**  
    在终端输入 `top`，查看实时的系统资源占用情况。
2. **找到罪魁祸首：**  
    按下 `P` 键（大写）按 CPU 使用率排序。找到 CPU 占用率异常高的 Java 进程，记下它的 **PID**（进程 ID）

### 2. 定位进程内消耗 CPU 的线程

1. **查看进程内线程：**  
    使用命令 `top -Hp <PID>`。这会列出该 Java 进程下所有的线程。
2. **找出高 CPU 线程：**  
    同样按 `P` 排序，找到 CPU 占用最高的线程 ID（记为 **TID**），并记录下来

### 3. 转换线程 ID 格式

JVM 的线程栈信息中，线程 ID 是以 **十六进制** 表示的，而 `top` 命令显示的是十进制。

- **执行转换：**  
    使用命令 `printf "%x\n" <TID>` 将十进制线程 ID 转换为十六进制（例如 `12346` 转换后是 `303a`）

### 4. 获取线程栈并定位代码

1. **导出线程栈：**  
    执行 `jstack <PID> > jstack.log`，将当前的线程堆栈信息保存到文件中。
2. **搜索关键线程：**  
    在文件中搜索刚才的十六进制线程 ID（如 `303a`）。
3. **分析状态：**
    - **RUNNABLE（运行中）：** 说明线程正在执行业务逻辑。查看堆栈信息，看它卡在哪个类、哪个方法、哪一行代码。
    - **BLOCKED（阻塞中）：** 说明线程在等待锁，可能存在锁竞争问题

### 常见原因与解决方案

**根据线程栈的分析结果，通常有以下几种常见情况：

- **死循环或高频计算：**
    - **现象：** 线程状态为 `RUNNABLE`，堆栈显示在某个 `while` 循环或复杂计算方法中。
    - **解决：** 检查循环退出条件，优化算法逻辑，或者在循环中加入 `Thread.sleep()` 防止空转。
- **正则表达式回溯爆炸：**
    - **现象：** 线程卡在 `java.util.regex.Pattern` 相关方法中。
    - **原因：** 使用了有性能缺陷的正则表达式处理了超长字符串。
    - **解决：** 优化正则表达式，或者改用字符串分割等简单匹配方式。
- **频繁 Full GC（假性 CPU 高）：**
    - **现象：** 线程栈里全是 `GC task thread`，或者 `jstat` 发现 GC 频率极高。
    - **原因：** 内存不足导致 JVM 疯狂回收垃圾。
    - **解决：** 这种情况通常伴随内存溢出风险，需要使用 `jstat -gcutil <PID>` 分析，排查内存泄漏或调整 JVM 参数。
- **锁竞争激烈：**
    - **现象：** 大量线程处于 `BLOCKED` 状态，都在等待同一个锁。
    - **解决：** 优化锁粒度，减少同步代码块范围，或者使用无锁数据结构


## 常用的问题排查命令

### jstat -gcutil

```shell
# 查看各区内存占用、GC情况
jstat -gcutil <pid> [interval] [count]

#示例 进程12345，每 2 秒打印一次，共打印 5 次 
stat -gcutil 12345 2000 5
```
- `<pid>`：Java 进程 ID（可通过 `jps` 查看）
- `[interval]`：采样间隔（单位：毫秒），例如 `1000` 表示每秒输出一次
- `[count]`：采样次数，可选；若不指定，则持续输出直到手动终止（Ctrl+C）

输出结果：
```shell
S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 
0.00 98.76 75.32 45.10 92.34 88.12 120 2.345 3 1.234 3.579
```

| 列名       | 含义                             | 说明                                                |
| -------- | ------------------------------ | ------------------------------------------------- |
| **S0**   | Survivor 0 区使用率 (%)            | 当前 Survivor 0 空间的使用百分比                            |
| **S1**   | Survivor 1 区使用率 (%)            | 当前 Survivor 1 空间的使用百分比  <br>（通常 S0 和 S1 只有一个在用）   |
| **E**    | Eden 区使用率 (%)                  | 新生代 Eden 区的使用百分比  <br>**频繁接近 100% → Young GC 频繁** |
| **O**    | Old 区（老年代）使用率 (%)              | **关键指标！持续增长且 Full GC 后不下降 → 内存泄漏**                |
| **M**    | Metaspace 使用率 (%)              | 元空间（JDK 8+）使用百分比  <br>（替代了永久代 PermGen）            |
| **CCS**  | Compressed Class Space 使用率 (%) | 压缩类指针空间使用率（与 M 相关）                                |
| **YGC**  | Young GC 次数                    | Minor GC 发生的总次数                                   |
| **YGCT** | Young GC 总耗时（秒）                | 所有 Young GC 累计耗时                                  |
| **FGC**  | Full GC 总耗时（秒）                 | 所有 Full GC 累计耗时                                   |
| **GCT**  | GC 总耗时（秒）                      | YGCT + FGCT                                       |

⚠️注意：不同 GC 算法（如 G1、ZGC、Shenandoah）的列名可能略有差异。例如 G1 中可能显示 `HU`（Humongous Used）等。

#### 场景 1：判断是否内存泄漏

- 观察 **O（Old Gen）** 列：
    - 如果 `O` 持续上升（如 60% → 70% → 80%...）
    - 即使发生 **Full GC（FGC 增加）**，`O` 也几乎不下降
    - → **极可能是内存泄漏**

#### 场景 2：判断 Young GC 是否过于频繁

- 观察 **E（Eden）** 列：
    - 如果 `E` 每隔几秒就从 0% 快速涨到 100%，然后归零（伴随 YGC +1）
    - 且 `YGCT` 增长很快
    - → 可能对象创建速率过高，或 Eden 区太小

#### 场景 3：判断是否频繁 Full GC（性能杀手）

- 观察 **FGC** 和 **FGCT**：
    - 如果 `FGC` 在短时间内快速增长（如 1 分钟内增加 10 次）
    - `FGCT` 占 `GCT` 大部分（如 >50%）
    - → 应用会严重卡顿，需立即排查老年代或元空间问题

#### 场景 4：观察元空间是否快满

- **M（Metaspace）** 接近 95%～100%
    - 可能动态生成类过多（如反射、CGLIB、Groovy 脚本）
    - 若继续增长可能触发 `Metaspace OOM`

### jmap -histo

```shell
# 统计大对象
jmap -histo [options] <pid>
```
- `<pid>`：目标 Java 进程 ID（可通过 `jps` 获取）
- `[options]`：
    - **不加选项**：只统计**存活对象**（live objects）
    - **`-all`**：统计**所有对象**（包括待回收的垃圾对象）

> ⚠️ 注意：执行 `jmap -histo` 会**触发一次 Full GC（仅限 `-histo`，不含 `-dump`）**，以确保统计的是“存活”对象。这会导致应用暂停（STW），**建议在业务低峰期使用**。

```
num #instances #bytes class name 
---------------------------------------------- 
1: 200000 16000000 java.lang.String 
2: 150000 12000000 java.util.HashMap$Node 
3: 50000 8000000 byte[] 
4: 10000 1600000 com.example.User
...
```

| 列名         | 含义                         |
| ---------- | -------------------------- |
| num        | 排名序号（按对象数量或内存降序）           |
| #instances | 该类的实例数量                    |
| #bytes     | 这些实例总共占用的字节数（Shallow Size） |
| class name | 类的全限定名                     |

## 有没有排查过JVM问题

首先通过 Pod 监控，发现服务在运行一段时间后周期性重启。  
进一步查看监控指标，发现每次重启前 CPU 使用率显著升高，最终导致容器宕机。  
经排查，宕机原因为 Kubernetes 主动终止（kill）了该 Pod，根本原因是 **CPU 资源耗尽导致健康检查接口（/health）无法在超时时间内返回**。

在下一次故障发生时，我们立即介入现场排查：

- 通过 `top` 定位到高 CPU 的 Java 进程 PID；
- 通过 `top -Hp <PID>` 找到占用 CPU 最高的线程 TID；
- 执行 `jstack <PID>` 导出线程堆栈，发现存在多个 **GC 工作线程（如 `G1 Young Generation`）处于 RUNNABLE 状态**；
- 使用 `jstat -gcutil <PID>` 确认 **老年代使用率持续 >95%**，且 GC 频率异常升高，表明存在严重内存压力；
- 随即执行 `jmap -dump:format=b,file=heap.hprof <PID>` 获取内存快照；
- 将 dump 文件导入 MAT 分析，发现一个 **Retained Heap 极大的 `ConcurrentHashMap` 实例**；
- 通过 **Path to GC Roots** 追踪引用链，确认该对象被 `ThreadLocal $ ThreadLocalMap` 中的 `Entry` 持有，且所属线程为线程池中的长期存活线程；

**最终结论**：由于业务代码在使用 `ThreadLocal` 存储 `ConcurrentHashMap` 后未调用 `remove()`，在线程池复用场景下导致内存泄漏，进而引发频繁 Full GC、CPU 飙升、健康检查失败，最终被 Kubernetes 终止。



